{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define base url for the scraper <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.indeed.com/\"\n",
    "job_search_url = base_url+\"jobs?q=AR%2FVR&l=Menlo+Park%2C+CA&start={}\"\n",
    "# a list to avoid scraping non relevant data\n",
    "job_key_words = [\"ar/vr\",\"augmented reality\",\"virtual reality\"]\n",
    "# list to store job titles\n",
    "job_title = []\n",
    "# list to store job posting urls\n",
    "jobs_main_url = []\n",
    "# set to store job posting urls to avoid inserting duplicate data\n",
    "temp_urls = set()\n",
    "# list to store company name\n",
    "company_names = []\n",
    "# list to store location, just incase if we are scraping in any other cities/country\n",
    "location = []\n",
    "# list to store job description\n",
    "job_description = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get the job listing urls and job title\n",
    "scrape first few pages of Indeed and get the job lising relevant to AR/VR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to scrape the main search page of indeed\n",
    "# get the job posting urls from the listing, i.e from the UL from html tag\n",
    "def extract_job_titles(soup): \n",
    "    for div in soup.find_all(name=\"h2\", attrs={\"class\":\"title\"}):\n",
    "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "            # if title is diffent than the one specified in the list of keywords\n",
    "            # move to next listing\n",
    "            if any(word in a[\"title\"].lower() for word in job_key_words):\n",
    "                post_url = base_url+a[\"href\"]\n",
    "                # just to ensure we do not have the job listing already in our list\n",
    "                if post_url not in temp_urls:\n",
    "#                     job_title.append(a[\"title\"])\n",
    "                    jobs_main_url.append(post_url)\n",
    "                    post_url = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=AR%2FVR&l=Menlo+Park%2C+CA&start=0\n",
      "https://www.indeed.com/jobs?q=AR%2FVR&l=Menlo+Park%2C+CA&start=10\n",
      "https://www.indeed.com/jobs?q=AR%2FVR&l=Menlo+Park%2C+CA&start=20\n",
      "https://www.indeed.com/jobs?q=AR%2FVR&l=Menlo+Park%2C+CA&start=30\n",
      "https://www.indeed.com/jobs?q=AR%2FVR&l=Menlo+Park%2C+CA&start=40\n",
      "https://www.indeed.com/jobs?q=AR%2FVR&l=Menlo+Park%2C+CA&start=50\n",
      "https://www.indeed.com/jobs?q=AR%2FVR&l=Menlo+Park%2C+CA&start=60\n",
      "https://www.indeed.com/jobs?q=AR%2FVR&l=Menlo+Park%2C+CA&start=70\n"
     ]
    }
   ],
   "source": [
    "# lets scrape first 8 pages of Indeed.com with the \n",
    "# specified keyword\n",
    "for i in range(0, 80, 10):\n",
    "    url_listing = job_search_url.format(i)\n",
    "    print(url_listing)\n",
    "    page = requests.get(url_listing)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    extract_job_titles(soup)\n",
    "# free up the memory by clearning the temp sets\n",
    "temp_urls.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.com//pagead/clk?mo=r&ad=-6NYlbfkN0DI_pqscLjs9LkB0jlO39g2s8RE9SCHTdataN4HV1TulGfWI13h5d2IIBkrMCBpdyoKFuk6LoRw93nv9VIf-nwycaiTnbsKlxaYmP2oUVCrEos5MOIWzflKyJqtBexCiKWVxNVq0D9Lo3YRyX8n959AonagBpNTwfxL-PurTe6VIuRucl5-GiO94yVWwnXAvVgISoGItfPrNseqMDsb4xC2ojEL91N8yKvVh4bd8Mc5fuIniD6hfFc5b-doaIZvhIa1uES5XVSvT3ZwSWYWbM26dFenN677y3wT3mAbQfeCx-eBrO3fZ60Mf6N8yo7O27bu3i_ACjkTtdABfxKxDVdGbAAiAKp5LueZVYo8A1yX4dEKjypmZ7SMejud5ZmCUl48R-WM6iV0GLzjxvtGle7gu1rXKaYhH5wem4w7C6VjphxN6S4ivBiFFK_odSQNZog5yGlaD_9buNb_h0eIbTwM&p=0&fvj=1&vjs=3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs_main_url)\n",
    "\n",
    "jobs_main_url[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clear the duplicates entries if we still got any\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "jobs_main_url = list(OrderedDict.fromkeys(jobs_main_url))\n",
    "len(jobs_main_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get the job description for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to scrape the inner job listing to\n",
    "# get the job description of that post\n",
    "def extract_job_description(soup): \n",
    "    jd_str = \"skills: \"\n",
    "    # job title\n",
    "    for div in soup.find_all(name=\"h3\", attrs={\"class\":\"icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title\"}):\n",
    "        job_title.append(div.text)\n",
    "    # company title\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"icl-u-lg-mr--sm icl-u-xs-mr--xs\"}):\n",
    "        if len(div.text.split('-')[0]) > 0:\n",
    "            company_names.append(div.text.split('-')[0])\n",
    "    # location\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"jobsearch-DesktopStickyContainer-companyrating\"}):\n",
    "        if len(div.text.split('-')[1]) > 2:\n",
    "            location.append(div.text.split('-')[1])\n",
    "    # Job Description\n",
    "#     for div in soup.find_all(name=\"div\", attrs={\"class\":\"jobsearch-jobDescriptionText\"}):\n",
    "#         job_description.append(div.text)\n",
    "    for div in soup.findAll('li'):\n",
    "        jd_str += \" \"+ div.text\n",
    "    job_description.append(jd_str)\n",
    "    jd_str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets scrape all the job urls we have in our list,\n",
    "# generate title,JD,location and comapny name for now\n",
    "for i in range(len(jobs_main_url)):\n",
    "    url_individual_listing = jobs_main_url[i]\n",
    "    page = requests.get(url_individual_listing)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    extract_job_description(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in jobs_main_url:\n",
    "#     page = requests.get(url)\n",
    "#     soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "#     extract_job_description(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80, 80, 80, 80)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# varify that all lists are of the same size \n",
    "# otherwise it will cause problem while creating a data frame\n",
    "len(job_title),len(jobs_main_url),len(job_description), len(location), len(company_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets save the scraped data as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame to save the raw data into a csv file \n",
    "#jb_url = list(jobs_main_url)\n",
    "raw_data = pd.DataFrame({\n",
    "    'Title' : job_title,\n",
    "    'Company': company_names,\n",
    "    'Location': location,\n",
    "    'JD': job_description,\n",
    "    'URL': jobs_main_url\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80 entries, 0 to 79\n",
      "Data columns (total 5 columns):\n",
      "Title       80 non-null object\n",
      "Company     80 non-null object\n",
      "Location    80 non-null object\n",
      "JD          80 non-null object\n",
      "URL         80 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>JD</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR/VR Business Analyst</td>\n",
       "      <td>Tailored Management</td>\n",
       "      <td>Menlo Park, CA</td>\n",
       "      <td>skills:  Consolidate third-party retailer sale...</td>\n",
       "      <td>https://www.indeed.com//pagead/clk?mo=r&amp;ad=-6N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Producer, Augmented Reality (Menlo Park)</td>\n",
       "      <td>The Mom Project</td>\n",
       "      <td>Menlo Park, CA</td>\n",
       "      <td>skills:  Work closely with creative leads to s...</td>\n",
       "      <td>https://www.indeed.com//pagead/clk?mo=r&amp;ad=-6N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title              Company  \\\n",
       "0                    AR/VR Business Analyst  Tailored Management   \n",
       "1  Producer, Augmented Reality (Menlo Park)      The Mom Project   \n",
       "\n",
       "         Location                                                 JD  \\\n",
       "0  Menlo Park, CA  skills:  Consolidate third-party retailer sale...   \n",
       "1  Menlo Park, CA  skills:  Work closely with creative leads to s...   \n",
       "\n",
       "                                                 URL  \n",
       "0  https://www.indeed.com//pagead/clk?mo=r&ad=-6N...  \n",
       "1  https://www.indeed.com//pagead/clk?mo=r&ad=-6N...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data into local disk\n",
    "raw_data.to_csv('raw_data.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
